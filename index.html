<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Chiyu Ma</title>
  
  <meta name="author" content="Chiyu Ma">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="Chiyu Ma - Ph.D. candidate at Dartmouth specializing in Large Scale LLM post-training, Reinforcement Learning and Supervised Fintuning.">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" href="images/dartmouth_logo.jpeg" type="image/x-icon"/>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Chiyu Ma</name>
              </p>
<p>
  I am Chiyu (Henry) Ma, a third-year Computer Science Ph.D. student at 
  <a href="https://home.dartmouth.edu/">Dartmouth ðŸŒ²</a>, 
  advised by <a href="https://www.cs.dartmouth.edu/~soroush/">ðŸŒŸ Prof. Soroush Vosoughi</a>. 
  Currently, I am also a Research Intern at Qwen Pilot, where I am focusing on research in large-scale post-training of Large Language Models (LLMs).
</p>
<p>
  Previously, I earned my M.S. in Statistical Science from Duke University. As a member of the Interpretable Machine Learning Lab, I was advised by 
  <a href="https://users.cs.duke.edu/~cynthia/">ðŸŒŸ Prof. Cynthia Rudin</a> 
  and collaborated with 
  <a href="https://mcec.umaine.edu/home/faculty222/chen-chaofan/">ðŸŒŸ Prof. Chaofan Chen</a> (UMaine). 
  I hold a B.S. in Statistics with Honors (Computational Finance concentration) from Carnegie Mellon University, where I conducted research on statistical analysis in biological applications with 
  <a href="https://sites.google.com/site/zjbranson/">ðŸŒŸ Prof. Zach Branson</a>.
</p>
              <p style="text-align:center">
                <a href="mailto:Chiyu.ma.gr@dartmouth.edu"><i class="fa fa-envelope" style="font-size:21px"></i></a> &nbsp/&nbsp
                <a href="https://https://github.com/Henrymachiyu"><i class="fa fa-github" style="font-size:23px"></i></a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/henry-chiyu-ma-3b7b30203/"><i class="fa fa-linkedin" style="font-size:22px"></i></a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=h_3TRv0AAAAJ&hl=en">Google Scholar</a>
              </p>
            </td>
            <td style="padding:2.5%;width:26%;max-width:26%">
              <a href="images/152928.jpg
"><img style="width:70%;max-width:70%" alt="profile photo" src="images/152928.jpg
" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
          <hr style="border:0; height:1px; background-image:linear-gradient(to right, transparent, #ddd, transparent);">

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">

              <heading>Research</heading>
<p>
My prior research centered on <b>Interpretability in Computer Vision Models (CNNs and ViTs)</b> 
  and ensuring the reliability of <b>LLM-as-a-Judge frameworks</b>. 
  Building on this foundation, I have pivoted to <b>Large-scale Post-training</b>, 
  where I design algorithms specifically aimed at eliciting and enhancing the complex reasoning capabilities of LLMs.            </td>
          </tr>
        </tbody></table>
<hr style="border:0; height:1px; background-image:linear-gradient(to right, transparent, #ddd, transparent);">



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">

                <heading>News</heading>
<p>
  <span style="font-style: italic;">[Jan. 28, 2026]</span> Two papers accepted to ICLR 2026. Thanks to my excellent collaborators. See you in Rio!
<br>
<p>
  <span style="font-style: italic;">[Sept. 2025]</span> Two papers accepted to EMNLP 2025 and One paper accepted to NeurIPS 2025. See you in Suzhou and SD!
<br>
<p>
  <span style="font-style: italic;">[Jun. 2025]</span> Started as a research intern at Tongyi Lab Qwen Pilot Team, Hangzhou, CA</a>. Excited to meet friends in Hangzhou!
<br>	

            </td>
          </tr>
        </tbody></table>

<hr style="border:0; height:1px; background-image:linear-gradient(to right, transparent, #ddd, transparent);">

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">

              <heading>Selected Publications</heading>
              <p style="font-size:95%;">
            	Some interesting papers about my work and research on LLM Post-training and Reinforcement Learning.
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr onmouseout="CMI_stop()" onmouseover="sgcl_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='fdnas_image'><video  width=100% height=100% muted autoplay loop>
                Your browser does not support the video tag.
                </video></div>
                <img src='images/logp.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<a href="https://openreview.net/forum?id=r6Pw3RiMYL">
                <papertitle> On the Direction of RLVR Updates for LLM Reasoning: Identification and Exploitation </papertitle>
              </a>
              <br>
              Kexin Huang, Haoming Meng, Junkang Wu, Jinda Lu, <b>Chiyu Ma</b>, Ziqian Chen, Xue Wang, Bolin Ding, Jiancan Wu, Xiang Wang, Xiangnan He, Guoyin Wang, Jingren Zhou
              <br>
                <em>ICLR 2026</em>
              <br>
                <a href="https://openreview.net/forum?id=r6Pw3RiMYL">Paper</a>
              <br>
              <p style="font-size:90%; margin-top:8px; color:#555;">
                This paper proposes that log-probability difference is a more promising metric than entropy for evaluating how LLMs evolve during training, a claim supported by both empirical and theoretical analysis.
              </p>
            </td>
          </tr>

	  <tr onmouseout="CMI_stop()" onmouseover="sgcl_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='fdnas_image'><video  width=100% height=100% muted autoplay loop>
                Your browser does not support the video tag.
                </video></div>
                <img src='images/sparse.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<a href="https://openreview.net/forum?id=8vWIXno8LW">
                <papertitle> Sparse but Critical: A Token-Level Analysis of Distributional Shifts in RLVR Fine-Tuning of LLMs </papertitle>
              </a>
              <br>
              Haoming Meng, Kexin Huang, Shaohang Wei, <b>Chiyu Ma</b>, Shuo Yang, Xue Wang, Guoyin Wang, Bolin Ding, Jingren Zhou 
              <br>
                <em>ICLR 2026</em> 
              <br>
               <a href="https://openreview.net/forum?id=8vWIXno8LW">Paper</a>
              <br>
              <p style="font-size:90%; margin-top:8px; color:#555;">
                Post-trained reasoning LLMs (across 3B, 8B, 70B) on MCTS-sampled data from physical simulators.
              </p>
            </td>
          </tr>

          <tr onmouseout="CMI_stop()" onmouseover="sgcl_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='fdnas_image'><video  width=100% height=100% muted autoplay loop>
                Your browser does not support the video tag.
                </video></div>
                <img src='images/video-captioner.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<a href="https://arxiv.org/pdf/2502.13363">
                <papertitle>â˜… Pretrained Image-Text Models are Secretly Video Captioners</papertitle>
              </a>
              <br>
              <b>Chunhui Zhang*</b>, Yiren Jian*, Zhongyu Ouyang, Soroush Vosoughi
	      <br>
	      <em>NAACL 2025 Main Conference</em> â€“ <strong>Oral Presentation (Top 2.88%)</strong>
	      <br>
              <a href="https://github.com/chunhuizng/mllm-video-captioner">Code</a> / <a href="https://arxiv.org/pdf/2502.13363">Paper</a> / <a href="data/naacl-video-captioner-slides.pdf">Slides</a>
              <br>
              <p style="font-size:90%; margin-top:8px; color:#555;">
                RL post-training recipe that achieved <b>Top-2</b> on PapersWithCode video captioning leaderboard, outperforming industry MLLM video captioners.
              </p>
            </td>
          </tr>
		  
	<tr onmouseout="CMI_stop()" onmouseover="sgcl_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='fdnas_image'><video  width=100% height=100% muted autoplay loop>
                Your browser does not support the video tag.
                </video></div>
                <img src='images/vla-emnlp2025.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<a href="">
                <papertitle>â˜… Knowing More, Acting Better: Hierarchical Representation for Embodied Decision-Making</papertitle>
              </a>
              <br>
              <b>Chunhui Zhang</b>, Zhongyu Ouyang, Xingjian Diao, Zheyuan Liu, Soroush Vosoughi
              <br>
                <em>EMNLP 2025 Findings</em>
		<br>
              <a href="data/EMNLP25_Embodied_Hierarchy.pdf">Paper</a>
              <br>
              <p style="font-size:90%; margin-top:8px; color:#555;">
                Refines vision-language-action LLM representations to enable more effective PPO-based RL training in embodied AI.
              </p>
            </td>
          </tr>

	<tr onmouseout="CMI_stop()" onmouseover="sgcl_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='fdnas_image'><video  width=100% height=100% muted autoplay loop>
                Your browser does not support the video tag.
                </video></div>
                <img src='images/working-memory.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<a href="">
                <papertitle>Working Memory Identifies Reasoning Limits in Language Models</papertitle>
              </a>
              <br>
              <b>Chunhui Zhang</b>, Yiren Jian, Zhongyu Ouyang, Soroush Vosoughi
              <br>
                <em>EMNLP 2024</em>
              <br>
              <a href="https://github.com/chunhuizng/working-memory-limits">Code</a> / <a href="data/EMNLP24_Working_Memory.pdf">Paper</a>
              <br>
              <p style="font-size:90%; margin-top:8px; color:#555;">
                Introduced working memory as a diagnostic tool for LLM reasoning limits. This work inspired a follow-up NAACL paper on long-context multimodal understanding.
              </p>
            </td>
          </tr>

    <tr onmouseout="CMI_stop()" onmouseover="sgcl_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/twm.png' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<a href="https://arxiv.org/pdf/2502.06020">
                <papertitle>Temporal Working Memory: Query-Guided Segment Refinement for Enhanced Multimodal Understanding</papertitle>
              </a>
              <br>
              Xingjian Diao*, <b>Chunhui Zhang*</b>, Weiyi Wu, Zhongyu Ouyang, Peijun Qing, Ming Cheng, Soroush Vosoughi, Jiang Gui
              <br>
                <em>NAACL 2025 Findings</em>
		<br>
              <a href="https://github.com/xid32/NAACL_2025_TWM">Code</a> / <a href="https://xid32.github.io/images/publications/Temporal_Working_Memory.pdf">Paper</a>
              <br>
              <p style="font-size:90%; margin-top:8px; color:#555;">
                Inspired by working memory in my EMNLP 2024 paper, this work is a follow-up study on long-context video-language understanding.
              </p>
            </td>
    </tr>
        </tbody></table>



<hr style="border:0; height:1px; background-image:linear-gradient(to right, transparent, #ddd, transparent);">

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected Collaborations</heading>
              <p style="font-size:90%;">
                Projects where I contributed as co-author. <a href="https://scholar.google.com.hk/citations?hl=en&user=jlqnbkAAAAAJ&view_op=list_works&sortby=pubdate">View all publications â†’</a>
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

	<tr onmouseout="CMI_stop()" onmouseover="sgcl_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/emnlp2025-soundmind.png' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<a href="https://arxiv.org/pdf/2506.12935">
                <papertitle>SoundMind: RL-Incentivized Logic Reasoning for Audio-Language Models</papertitle>
              </a>
              <br>
              Xingjian Diao, <b>Chunhui Zhang</b>, Keyi Kong, Weiyi Wu, Chiyu Ma, Zhongyu Ouyang, Peijun Qing, Soroush Vosoughi, Jiang Gui
              <br>
                <em>EMNLP 2025</em> â€“ <strong>Oral Presentation</strong>
              <br>
              <a href="https://arxiv.org/pdf/2506.12935">Paper</a>
            </td>
          </tr>

	<tr onmouseout="CMI_stop()" onmouseover="sgcl_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/reasoning.png' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<a href="https://arxiv.org/pdf/2503.02103">
                <papertitle>Superficial Self-Improved Reasoners Benefit from Model Merging</papertitle>
              </a>
              <br>
              Xiangchi Yuan, <b>Chunhui Zhang</b>, Zheyuan Liu, Dachuan Shi, Soroush Vosoughi, Wenke Lee
              <br>
                <em>EMNLP 2025</em>
              <br>
              <a href="https://github.com/xiangchi-yuan/merge_syn">Code</a> / <a href="https://arxiv.org/pdf/2503.02103">Paper</a>
            </td>
          </tr>

        <tr onmouseout="CMI_stop()" onmouseover="sgcl_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/vlm.png' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<a href="https://arxiv.org/abs/2310.03291">
                <papertitle>Expedited Training of Visual Conditioned Language Generation via Redundancy Reduction</papertitle>
              </a>
              <br>
              Yiren Jian, Tingkai Liu, Yunzhe Tao, <b>Chunhui Zhang</b>, Soroush Vosoughi, Hongxia Yang
              <br>
                <em>ACL 2024</em> â€“ <strong>Oral Presentation (Top 3.10%)</strong>
              <br>
              <a href="https://github.com/yiren-jian/EVLGen">Code</a> / <a href="https://arxiv.org/abs/2310.03291">Paper</a>
            </td>
          </tr>

        <tr onmouseout="CMI_stop()" onmouseover="sgcl_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/datadec.png' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<a href="https://openreview.net/forum?id=3jV525Hmqr">
                <papertitle>When Sparsity Meets Contrastive Models: Less Data Can Bring Better Class-Balanced Representations</papertitle>
              </a>
              <br>
              <b>Chunhui Zhang</b>, Chao Huang, Yijun Tian, Qianlong Wen, Zhongyu Ouyang, Youhuan Li, Yanfang Ye, et al.
              <br>
        <em>ICML 2023</em> â€“ <strong>AAAI-DCAA 2023 Best Paper Runner-up Award</strong>
              <br>
              <a href="https://openreview.net/forum?id=3jV525Hmqr">Paper</a>
            </td>
          </tr>

        <tr onmouseout="CMI_stop()" onmouseover="sgcl_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/iclr_game.png' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<a href="https://openreview.net/pdf?id=7jk5gWjC18M">
                <papertitle>Chasing All-Round Graph Representation Robustness: Model, Training, and Optimization</papertitle>
              </a>
              <br>
              <b>Chunhui Zhang</b>, Yijun Tian, Mingxuan Ju, Zheyuan Liu, Yanfang Ye, Nitesh Chawla, et al.
              <br>
        <em>ICLR 2023</em>
              <br>
              <a href="https://github.com/chunhuizng/GAME">Code</a> / <a href="https://openreview.net/pdf?id=7jk5gWjC18M">Paper</a>
            </td>
          </tr>

        <tr onmouseout="CMI_stop()" onmouseover="sgcl_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/lip_align.png' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<a href="https://github.com/chunhuizng/lipschitz-fairness">
                <papertitle>Aligning Relational Learning with Lipschitz Fairness</papertitle>
              </a>
              <br>
              Yaning Jia*, <b>Chunhui Zhang*</b>, Soroush Vosoughi
              <br>
                <em>ICLR 2024</em> (<i>*co-first author</i>)
              <br>
              <a href="https://github.com/chunhuizng/lipschitz-fairness">Code</a> / <a href="data/ICLR24_Lipschitz_Align_camera_ready.pdf">Paper</a>
            </td>
          </tr>

        <tr onmouseout="CMI_stop()" onmouseover="sgcl_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/emergent-degradation.png' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<a href="https://openreview.net/forum?id=Koh0i2u8qX">
                <papertitle>Mitigating Emergent Robustness Degradation on Graphs while Scaling Up</papertitle>
              </a>
              <br>
              Xiangchi Yuan*, <b>Chunhui Zhang*</b>, Yijun Tian, Yanfang Ye, et al.
              <br>
                <em>ICLR 2024</em> (<i>*co-first author</i>)
              <br>
              <a href="https://github.com/chunhuizng/emergent-degradation">Code</a> / <a href="https://openreview.net/forum?id=Koh0i2u8qX">Paper</a>
            </td>
          </tr>

        </tbody></table>

<hr style="border:0; height:1px; background-image:linear-gradient(to right, transparent, #ddd, transparent);">

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>To All Students/Friends</heading>
              <p>
Feel free to drop me an email if you're up for a laid-back chat about life, career, or research. I'm dedicating (at least) 30 minutes each week for these discussions, and I'm particularly eager to connect with students from underrepresented backgrounds or those dealing with challenges or inequity. Just reach outâ€”I'm here to listen!
              </p>
            </td>
          </tr>
        </tbody></table>

<hr style="border:0; height:1px; background-image:linear-gradient(to right, transparent, #ddd, transparent);">

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Fun Stuff</heading>
              <p>
                Racingâ€”a happy part of my life. I particularly enjoy go-karting and circuit racing (some fun facts:
                  <a href="https://www.instagram.com/p/Ce-IBtFsBUl/">1st</a> and <a href="https://www.instagram.com/p/CgV3251sPcJ/">2nd</a> place at Supercharged). But there is one type of racing that I have yet to tryâ€”my favorite rally driving (My favorite rally driver is Han Han).
              </p>
            </td>
          </tr>
        </tbody></table>

<script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=IMj5QOauaOh3itXiv54K4YbC6WTpio8QQs1f5DGrnzo&cl=ffffff&w=120&h=80"></script>
<div style="height:1px; margin-top:20px; margin-bottom:20px; background-image:linear-gradient(to right, transparent, #ddd, transparent);"></div>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Design and source code from <a href="https://github.com/jonbarron/jonbarron_website">this cool guy</a>.
                <br>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
